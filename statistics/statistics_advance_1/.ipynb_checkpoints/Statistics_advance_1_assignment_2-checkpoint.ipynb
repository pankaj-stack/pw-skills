{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "592bd343-bcb8-4de4-a316-d0560bc04375",
   "metadata": {},
   "source": [
    "### Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbc5fb1-45f2-4196-aa52-c33be8458c7f",
   "metadata": {},
   "source": [
    "**Probability Mass Function (PMF) and Probability Density Function (PDF):** \n",
    "\n",
    "Probability Mass Function (PMF) and Probability Density Function (PDF) are mathematical concepts used in probability theory to describe the distribution of random variables. They play a crucial role in understanding the likelihood of different outcomes in both discrete and continuous probability distributions, respectively.\n",
    "\n",
    "**Probability Mass Function (PMF):** PMF is used to describe the distribution of discrete random variables. It assigns probabilities to individual values of a discrete random variable, indicating the likelihood of each possible outcome. In other words, the PMF provides the probability of a specific value occurring in a discrete random variable. The sum of probabilities across all possible values equals 1. PMF is denoted as \\(P(X = x)\\), where \\(X\\) is the random variable and \\(x\\) is a specific value within the range of \\(X\\). An example of a PMF could be the probability distribution of rolling a fair six-sided die, where each face has a probability of \\(1/6\\).\n",
    "\n",
    "**Probability Density Function (PDF):** PDF is used to describe the distribution of continuous random variables. Unlike discrete random variables, continuous random variables can take on an infinite number of values within a range. The PDF assigns probabilities to intervals of values, rather than specific values. It indicates the relative likelihood of a continuous random variable falling within a particular range of values. The integral of the PDF over an interval gives the probability of the random variable falling within that interval. The integral of the entire PDF over the entire range equals 1. PDF is denoted as \\(f(x)\\), where \\(x\\) is a specific value within the range of the continuous random variable. An example of a PDF could be the distribution of weights of hamburgers from a fast-food chain, where the probability of a specific weight interval is given by the PDF\n",
    "\n",
    "**Example: Rolling a Die vs. Hamburger Weights:**\n",
    "To illustrate PMF and PDF, consider the example of rolling a fair six-sided die and the distribution of weights of hamburgers.\n",
    "\n",
    "1. **Rolling a Die (Discrete Random Variable):** In this case, the PMF would define the probability distribution for the discrete random variable \\(X\\) representing the outcome of rolling the die. Each face has a probability of \\(1/6\\) of being rolled, as it's a fair die. The PMF would provide the probabilities for each face value (1 to 6) individually.\n",
    "\n",
    "2. **Hamburger Weights (Continuous Random Variable):** For the distribution of hamburger weights, the PDF would describe the probabilities of different weight intervals. Let's say the weights are uniformly distributed between \\(0.2\\) and \\(0.3\\) pounds. The PDF would be a constant function over this interval, indicating that any weight within that interval has an equal likelihood.\n",
    "\n",
    "In both cases, the concepts of PMF and PDF help us understand the probabilities associated with different outcomes in the respective distributions.\n",
    "\n",
    "In summary, PMF is used for discrete random variables, providing probabilities for individual values, while PDF is used for continuous random variables, providing probabilities for intervals of values. Both PMF and PDF are fundamental concepts in probability theory that help us analyze and describe random phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331392c-77f2-4990-abfe-f73913dbb2a4",
   "metadata": {},
   "source": [
    "# Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87724382-5a36-4b83-a2b7-eb2a9f082085",
   "metadata": {},
   "source": [
    "**Cumulative Distribution Function (CDF):**\n",
    "\n",
    "The Cumulative Distribution Function (CDF) is a concept used in probability theory and statistics to describe the distribution of random variables. It provides information about the probabilities of random variables taking on values less than or equal to a given value. The CDF is a fundamental tool for understanding the behavior of random variables and their probabilities.\n",
    "\n",
    "The CDF is denoted by \\(F_X(x)\\) and is defined as the probability that a random variable \\(X\\) takes on a value less than or equal to \\(x\\). In mathematical notation:\n",
    "\\[F_X(x) = P(X \\leq x), \\text{ for all } x \\in \\mathbb{R}.\\]\n",
    "Here, \\(X\\) is the random variable, and \\(x\\) is a specific value within the range of \\(X\\)\n",
    "\n",
    "**Example of Cumulative Distribution Function:**\n",
    "\n",
    "Let's consider the example of rolling a fair six-sided die. The random variable \\(X\\) represents the outcome of rolling the die. The CDF \\(F_X(x)\\) for this random variable would provide the probabilities of getting a value less than or equal to \\(x\\) when rolling the die.\n",
    "\n",
    "For each value \\(x\\) from 1 to 6, \\(F_X(x)\\) would be the probability of rolling a number less than or equal to \\(x\\). Since all sides of the die are equally likely, each \\(F_X(x)\\) would be \\(1/6\\) until \\(x\\) reaches 6. For example:\n",
    "- \\(F_X(1) = P(X \\leq 1) = 1/6\\)\n",
    "- \\(F_X(2) = P(X \\leq 2) = 2/6 = 1/3\\)\n",
    "- \\(F_X(3) = P(X \\leq 3) = 3/6 = 1/2\\)\n",
    "- \\(F_X(4) = P(X \\leq 4) = 4/6 = 2/3\\)\n",
    "- \\(F_X(5) = P(X \\leq 5) = 5/6\\)\n",
    "- \\(F_X(6) = P(X \\leq 6) = 6/6 = 1\\)\n",
    "\n",
    "**Why is CDF Used?**\n",
    "\n",
    "The CDF is used for several reasons:\n",
    "\n",
    "1. **Quantifying Probabilities:** The CDF provides a way to calculate probabilities associated with random variables. It helps us understand the likelihood of a random variable falling within a certain range.\n",
    "\n",
    "2. **Comparing Distributions:** The CDF allows easy comparison between different probability distributions. It provides a visual representation of the distribution's behavior and characteristics.\n",
    "\n",
    "3. **Statistical Analysis:** CDFs are used for various statistical analyses, including hypothesis testing and confidence interval calculations.\n",
    "\n",
    "4. **Decision Making:** CDFs are valuable in decision-making processes involving uncertainty. They provide insights into the possible outcomes and their probabilities.\n",
    "\n",
    "In summary, the Cumulative Distribution Function is a powerful tool in probability and statistics that provides information about the probabilities of random variables taking on specific values or falling within certain ranges. It is widely used for analyzing and understanding random phenomena and making informed decisions based on uncertainty.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43baf64c-12b3-4d95-9350-69986b2120ff",
   "metadata": {},
   "source": [
    "# Q3: What are some examples of situations where the normal distribution might be used as a model? Explain how the parameters of the normal distribution relate to the shape of the distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b9dc9-21b7-4b5a-9ec0-26ce5d422ee4",
   "metadata": {},
   "source": [
    "The normal distribution, also known as the Gaussian distribution or bell curve, is a commonly used statistical model in various fields due to its symmetrical and characteristic bell-shaped curve. Here are some examples of situations where the normal distribution might be used as a model:\n",
    "\n",
    "1. **Height:** The height of people in a specific population often follows a normal distribution. Most individuals are of average height, with a roughly equal number of people being taller or shorter than average.\n",
    "\n",
    "2. **Birth Weight:** The birth weight of newborn babies is another example that often exhibits a normal distribution. It's well-documented that the birth weight of newborns follows a bell-shaped distribution with a mean around 7.5 pounds.\n",
    "3. **Reading Ability:** Reading ability, as well as other cognitive traits like IQ scores, often follow a normal distribution. This allows statisticians and researchers to make comparisons between different groups and estimate population characteristics using sample data.\n",
    "\n",
    "4. **Job Satisfaction:** Variables related to job satisfaction can also be modeled using the normal distribution. Understanding the properties of normal distributions helps in conducting inferential statistics to analyze and make estimates about different groups.\n",
    "\n",
    "5. **SAT Scores:** Standardized test scores like SAT scores often follow a normal distribution. This is why many statistical tests are designed for populations that exhibit a normal distribution.\n",
    "\n",
    "6. **Physical Characteristics:** Various physical characteristics such as shoe sizes, weights, and other measures related to the human body can be modeled using the normal distribution.\n",
    "\n",
    "7. **Financial Analysis:** In finance, the normal distribution is used to model stock market returns and various financial variables. It's a key concept in understanding risk and return relationships in investments.\n",
    "\n",
    "8. **Quality Control:** The normal distribution is used in quality control processes to assess the variability and consistency of products. This helps in setting acceptable limits for defects and deviations in manufacturing processes\n",
    "\n",
    "9. **Sampling Distributions:** The central limit theorem states that the distribution of sample means from any population will be approximately normally distributed as the sample size increases. This principle is crucial for drawing statistical inferences and making estimations about population parameters.\n",
    "\n",
    "10. **Probability Problems:** The normal distribution is often used as a standard of reference for many probability problems due to its widespread applicability in approximating natural phenomena.\n",
    "\n",
    "These are just a few examples of situations where the normal distribution serves as a valuable model for understanding and analyzing various types of data. Its symmetrical and predictable characteristics make it a powerful tool in statistics and data analysis.\n",
    "\n",
    "\n",
    "The shape of a normal distribution is primarily determined by its two main parameters: the mean and the standard deviation. These parameters play a crucial role in defining the characteristics and appearance of the distribution curve.\n",
    "\n",
    "1. **Mean (μ):** The mean of a normal distribution serves as a measure of central tendency. It determines the point around which the distribution is centered. In a symmetric normal distribution, the mean is also the point of highest probability, and the curve is perfectly symmetrical around this point. As the mean shifts along the horizontal axis, the entire distribution shifts accordingly. For example, increasing the mean value shifts the entire curve to the right, while decreasing it shifts the curve to the left. The mean also dictates the location of the peak of the bell-shaped curve.\n",
    "\n",
    "2. **Standard Deviation (σ):** The standard deviation of a normal distribution measures the spread or dispersion of the data points from the mean. A smaller standard deviation results in a narrower and taller curve, while a larger standard deviation creates a broader and flatter curve. In essence, the standard deviation determines the \"width\" of the bell-shaped curve. A higher standard deviation indicates greater variability in the data, leading to a wider distribution. Conversely, a lower standard deviation indicates less variability, resulting in a narrower distribution. The relationship between the standard deviation and the shape of the curve is inversely proportional.\n",
    "\n",
    "In summary, the mean determines the central point of the distribution, while the standard deviation controls the spread or width of the distribution. Together, these parameters define the bell-shaped curve that characterizes the normal distribution. Changes in these parameters result in shifts, expansions, or contractions of the curve, ultimately influencing how the data is distributed around the mean and affecting the probabilities associated with different values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5676817-7d5d-4d16-abd4-627d3c8814bd",
   "metadata": {},
   "source": [
    "# Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal Distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f51fee-f07e-4e74-b1e3-8577e7570721",
   "metadata": {},
   "source": [
    "The Normal Distribution, also known as the Gaussian Distribution or the Bell Curve, holds significant importance in various fields due to its unique properties and characteristics. Here are some key reasons why the Normal Distribution is important:\n",
    "\n",
    "1. **Central Limit Theorem and Approximation:** The Normal Distribution is closely tied to the Central Limit Theorem, which states that the sum or average of a large number of independent, identically distributed random variables approaches a normal distribution, even if the original variables themselves are not normally distributed. This property makes the Normal Distribution a fundamental concept in statistics, allowing us to approximate the distributions of various phenomena even when the underlying data may not be normally distributed.\n",
    "\n",
    "2. **Universal Representation:** Many natural and social phenomena exhibit a distribution that approximates the Normal Distribution. Variables such as heights, weights, IQ scores, measurement errors, and even financial returns often follow this pattern. This makes the Normal Distribution a common choice for modeling and understanding real-world data.\n",
    "\n",
    "3. **Statistical Inference:** The properties of the Normal Distribution simplify statistical inference. It is used as the foundation for many statistical tests and techniques, as its symmetry and well-defined characteristics make calculations and interpretations more straightforward. This simplification is especially important for hypothesis testing, confidence intervals, and regression analysis .\n",
    "\n",
    "4. **Standardization and Z-Scores:** The concept of standardization using Z-scores (measuring the number of standard deviations a data point is from the mean) is based on the properties of the Normal Distribution. This standardization allows for meaningful comparisons between different data sets and variables, facilitating data analysis and interpretation .\n",
    "\n",
    "5. **Financial Analysis:** In finance, the Normal Distribution is used to model stock price movements, asset returns, and risk assessment. It helps financial analysts and investors make predictions and decisions based on historical and expected market behavior .\n",
    "\n",
    "6. **Quality Control and Process Improvement:** In manufacturing and quality control, the Normal Distribution is often used to model variability in processes. Deviations from the mean can indicate defects or issues in the production process, allowing for targeted improvements .\n",
    "\n",
    "7. **Education and Psychology:** Many educational and psychological measurements, such as test scores and IQ scores, tend to follow a Normal Distribution. This allows educators and psychologists to set standards, assess performance, and make informed decisions about educational programs and interventions.\n",
    "\n",
    "8. **Risk Management:** In fields like insurance, risk assessment involves understanding the distribution of potential losses. The Normal Distribution helps assess the likelihood of extreme events and set appropriate insurance premiums .\n",
    "\n",
    "Real-life examples of the Normal Distribution include:\n",
    "\n",
    "- **Height of People:** The height of individuals in a population tends to follow a Normal Distribution, with most people clustered around the average height.\n",
    "- **Financial Returns:** The fluctuations in stock prices and financial returns often exhibit characteristics of the Normal Distribution, helping analysts understand and predict market behavior .\n",
    "\n",
    "- **IQ Scores:** IQ scores among a population also tend to approximate a Normal Distribution, allowing educators to understand the distribution of intelligence levels.\n",
    "\n",
    "- **Measurement Errors:** Errors in measurements, such as those in scientific experiments or industrial processes, often follow a Normal Distribution, aiding in error analysis and correction.\n",
    "\n",
    "In summary, the Normal Distribution's significance lies in its ability to model and approximate a wide range of natural and social phenomena, simplifying statistical analysis, decision-making, and understanding variability in various fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dedf8f-5263-48c9-bd95-67b695321b98",
   "metadata": {},
   "source": [
    "# Q8: What is the z score? State the importance of the z score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e6f87-9e8b-49b0-a4dd-9e0566197a2b",
   "metadata": {},
   "source": [
    "The **z-score**, also known as the **standard score**, is a statistical measurement that indicates the relationship between \n",
    "a particular data point and the mean of a group of values. It's calculated in terms of standard deviations \n",
    "from the mean. **A z-score of 0 signifies that the data point's value is identical to the mean**, while a positive \n",
    "z-score indicates that the value is above the mean, and a negative z-score indicates that the value is below the\n",
    "mean. **Z-scores are particularly important because they allow for the standardization and comparison of values\n",
    "within a distribution**. They help you understand where a specific data point fits in the distribution, compare \n",
    "observations from different variables, **identify outliers**, and calculate probabilities and percentiles using the \n",
    "standard normal distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef7de42-8638-4d99-bc7b-9506b1e99331",
   "metadata": {},
   "source": [
    "# Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dc8552-f2c4-4ab7-a4f3-f4d7acea2c17",
   "metadata": {},
   "source": [
    "The Central Limit Theorem (CLT) is a fundamental concept in statistics that states that the distribution of the sample means of a sufficiently large number of independent random samples drawn from a population will approximate a normal distribution, regardless of the original population's distribution shape. In other words, even if the individual data points are not normally distributed, the distribution of the sample means will tend to become more bell-shaped (normal) as the sample size increases. The theorem holds true under certain conditions, such as when the sample size is sufficiently large.\n",
    "\n",
    "The significance of the Central Limit Theorem lies in its broad applicability and its role in statistical inference. Here are some key points regarding the importance of the Central Limit Theorem:\n",
    "\n",
    "1. **Approximation to Normal Distribution**: The CLT allows statisticians and data analysts to work with the assumption of a normal distribution for sample means, which simplifies many statistical analyses and calculations. This is essential because the normal distribution is well understood and characterized mathematically, making it easier to perform various statistical tests and make inferences about a population.\n",
    "\n",
    "2. **Statistical Inference**: The CLT underpins many statistical inference techniques, such as hypothesis testing and confidence intervals. These techniques are used to draw conclusions about a population based on a sample. The assumption of normality in the sampling distribution of the mean is crucial for the validity of these methods.\n",
    "\n",
    "3. **Small Sample Size**: Even when dealing with populations that may not be normally distributed, the CLT provides a practical way to work with sample means and their distributions. This is especially useful when the sample size is small and assumptions of normality can't be reliably met.\n",
    "\n",
    "4. **Real-World Applications**: The CLT is widely used in various fields such as economics, biology, physics, social sciences, and more. It allows researchers and analysts to make inferences about populations based on limited samples, which is often the case in real-world scenarios.\n",
    "\n",
    "5. **Sampling Variability**: The CLT helps us understand the distribution of sample means and the inherent variability between different samples drawn from the same population. This knowledge is essential for assessing the reliability and stability of our findings.\n",
    "\n",
    "6. **Model Validation**: The CLT can also be used to validate certain assumptions about a population's distribution. If the distribution of sample means closely resembles a normal distribution, it suggests that the assumptions underlying the analysis may be reasonable.\n",
    "\n",
    "In summary, the Central Limit Theorem is a cornerstone of statistical theory that allows us to work with sample means as if they were normally distributed, even when dealing with populations that may not follow a normal distribution. This property is essential for performing various statistical analyses, making inferences, and drawing conclusions in a wide range of fields."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa6fb69-916e-4a66-ac0e-39ef4df0c184",
   "metadata": {},
   "source": [
    "# Q10: State the assumptions of the Central Limit Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88baf3a3-7624-4af0-8b7a-f1371e5dfe9a",
   "metadata": {},
   "source": [
    "The assumptions of the Central Limit Theorem are as follows:\n",
    "\n",
    "1. **Independence Assumption**: The data values within each sample and across different samples must be independent of each other. This means that the values should not be influenced by or related to each other. In other words, the observations in one sample should not affect the observations in another sample. This assumption is important because the theorem relies on the idea of averaging independent observations.\n",
    "\n",
    "2. **Finite Variance**: The population from which the samples are drawn should have a finite variance. Variance measures the spread or dispersion of data points from the mean. If the variance is infinite or undefined, the conditions for the Central Limit Theorem may not hold. In practical terms, a population with a reasonably well-defined and finite variance will satisfy this assumption.\n",
    "\n",
    "3. **Sample Size**: While not always explicitly stated as an assumption, the Central Limit Theorem tends to work better with larger sample sizes. Although there is no strict rule for the minimum sample size, a general guideline is that the sample size should be reasonably large, often suggested to be equal to or greater than 30. Larger sample sizes lead to a more accurate approximation to the normal distribution for the sample means.\n",
    "\n",
    "4. **Random Sampling**: The samples should be selected randomly from the population. This means that every member of the population should have an equal probability of being included in the sample. Random sampling helps ensure that the samples are representative of the population and that the results are generalizable.\n",
    "\n",
    "These assumptions collectively contribute to the conditions under which the Central Limit Theorem holds true. If any of these assumptions are violated, the Central Limit Theorem might not be applicable or may not provide accurate results. However, in practice, the Central Limit Theorem often works well even when the assumptions are only approximately met, especially as the sample size becomes larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8149be75-b849-45ba-a1eb-98b415e035d8",
   "metadata": {},
   "source": [
    "# Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli Distribution and Binomial Distribution?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ea8b61-fa28-4e3a-a383-70378fc0f18e",
   "metadata": {},
   "source": [
    "**Bernoulli Distribution:**\n",
    "\n",
    "The Bernoulli distribution is a discrete probability distribution that describes a random variable with two possible outcomes, typically labeled as \"success\" and \"failure,\" often represented as 1 and 0, respectively. It models situations where there are only two possible outcomes, and it's characterized by a single parameter, usually denoted as \\(p\\), which represents the probability of success.\n",
    "\n",
    "**Example of Bernoulli Distribution:**\n",
    "\n",
    "An example of a Bernoulli distribution is flipping a fair coin. Let's say we define \"success\" as getting heads and \"failure\" as getting tails. If the probability of getting heads (\\(p\\)) is 0.5 (assuming a fair coin), then the probability of getting tails (\\(1-p\\)) is also 0.5. Thus, the random variable \\(X\\) that represents this situation follows a Bernoulli distribution with parameter \\(p = 0.5\\). The probability mass function of this distribution is as follows:\n",
    "\n",
    "\\[P(X = x) = \\begin{cases}\n",
    "p & \\text{if } x = 1 \\\\\n",
    "1 - p & \\text{if } x = 0\n",
    "\\end{cases}\\]\n",
    "\n",
    "**Difference between Bernoulli Distribution and Binomial Distribution:**\n",
    "\n",
    "1. **Number of Trials:**\n",
    "   - Bernoulli Distribution: It describes a single trial with two possible outcomes.\n",
    "   - Binomial Distribution: It describes the number of successes in a fixed number of independent Bernoulli trials.\n",
    "\n",
    "2. **Number of Outcomes:**\n",
    "   - Bernoulli Distribution: Only two possible outcomes (success and failure).\n",
    "   - Binomial Distribution: Multiple possible outcomes (number of successes ranging from 0 to the total number of trials).\n",
    "\n",
    "3. **Parameter:**\n",
    "   - Bernoulli Distribution: Characterized by a single parameter \\(p\\) (probability of success).\n",
    "   - Binomial Distribution: Characterized by two parameters: \\(n\\) (number of trials) and \\(p\\) (probability of success).\n",
    "\n",
    "4. **Random Variable:**\n",
    "   - Bernoulli Distribution: Represents the outcome of a single trial (1 or 0).\n",
    "   - Binomial Distribution: Represents the number of successes in \\(n\\) trials (0, 1, 2, ..., \\(n\\)).\n",
    "\n",
    "5. **Probability Mass Function:**\n",
    "   - Bernoulli Distribution: \\(P(X = x) = p^x \\cdot (1 - p)^{1-x}\\) for \\(x = 0, 1\\).\n",
    "   - Binomial Distribution: \\(P(X = k) = \\binom{n}{k} \\cdot p^k \\cdot (1 - p)^{n - k}\\) for \\(k = 0, 1, ..., n\\).\n",
    "\n",
    "6. **Relationship:**\n",
    "   - A single Bernoulli trial is equivalent to a binomial distribution with \\(n = 1\\).\n",
    "\n",
    "In summary, the Bernoulli distribution models a single trial with two outcomes, while the binomial distribution models the number of successes in a fixed number of independent trials, each following a Bernoulli distribution. The binomial distribution generalizes the Bernoulli distribution to scenarios with multiple trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4c2c74-f9c0-41bd-b81c-40d566400efe",
   "metadata": {},
   "source": [
    "# Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset is normally distributed, what is the probability that a randomly selected observation will be greaterthan 60? Use the appropriate formula and show your calculations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd65cd9-e1a1-4445-b05c-1e13af51d971",
   "metadata": {},
   "source": [
    "To calculate the probability that a randomly selected observation from a normally distributed dataset with a mean of 50 and a standard deviation of 10 will be greater than 60, you can use the cumulative distribution function (CDF) of the normal distribution. The CDF gives the probability that a value is less than or equal to a given value. Since you want the probability that the value is greater than 60, you can subtract the CDF value at 60 from 1.\n",
    "\n",
    "The formula for the CDF of the normal distribution is as follows:\n",
    "\\[P(X \\leq x) = \\Phi\\left(\\frac{x - \\mu}{\\sigma}\\right)\\]\n",
    "Where:\n",
    "- \\(X\\) is the random variable\n",
    "- \\(x\\) is the value for which you want to calculate the probability\n",
    "- \\(\\mu\\) is the mean of the distribution (50 in this case)\n",
    "- \\(\\sigma\\) is the standard deviation of the distribution (10 in this case)\n",
    "- \\(\\Phi\\) is the cumulative distribution function of the standard normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fa9404-dccc-4f29-9f66-fb77d9d5e64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability that a randomly selected observation will be greater than 60 is: 0.1587\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "mean = 50\n",
    "std_dev = 10\n",
    "value = 60\n",
    "\n",
    "# Calculate the z-score\n",
    "z_score = (value - mean) / std_dev\n",
    "\n",
    "# Calculate the probability using the cumulative distribution function (CDF)\n",
    "probability = 1 - scipy.stats.norm.cdf(z_score)\n",
    "\n",
    "print(f\"The probability that a randomly selected observation will be greater than 60 is: {probability:.4f}\")\n",
    "\n",
    "# Using the formula and the provided information, the code calculates the z-score for the value 60, then uses the \n",
    "# cumulative distribution function to find the probability that a randomly selected observation is less than or equal\n",
    "# to 60, and finally subtracts that probability from 1 to get the probability that the observation is greater than 60. The result will be printed out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2816691-9f70-4478-90c1-700687809288",
   "metadata": {},
   "source": [
    "# Q7: Explain uniform Distribution with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51967f78-9bc4-44b1-853b-4dd276f00a6c",
   "metadata": {},
   "source": [
    "The uniform distribution is a probability distribution that describes the likelihood of an outcome occurring within a certain range. In a uniform distribution, every possible outcome within the range has an equal probability of occurring. This distribution is often visualized as a flat and constant line over the entire range, indicating that all outcomes are equally likely.\n",
    "\n",
    "\n",
    "Imagine rolling a fair six-sided die. Each face of the die has a number from 1 to 6. If the die is fair, then each number has an equal chance of coming up when you roll the die. This situation can be modeled using a uniform distribution.\n",
    "\n",
    "\n",
    "In this case, the range of possible outcomes is from 1 to 6, and since the die is fair, the probability of rolling any specific number is 1/6. The probability density function (PDF) of a uniform distribution can be written as:\n",
    "\n",
    "\n",
    "code\n",
    "f(x) = 1 / (b - a) for a ≤ x ≤ b\n",
    "\n",
    "Where:\n",
    "\n",
    "\n",
    "a is the minimum value in the range (1 for the die example).\n",
    "b is the maximum value in the range (6 for the die example).\n",
    "f(x) is the probability density function.\n",
    "\n",
    "For the fair six-sided die example:\n",
    "\n",
    "\n",
    "a = 1\n",
    "b = 6\n",
    "Therefore, f(x) = 1 / (6 - 1) = 1/6\n",
    "\n",
    "This means that each number on the die has a probability of 1/6 of being rolled, and this probability remains constant for all outcomes within the range.\n",
    "\n",
    "\n",
    "In summary, the uniform distribution is a simple and intuitive probability distribution where all outcomes within a range are equally likely. The example of rolling a fair six-sided die helps illustrate this concept, as each number has an equal chance of being rolled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0868ace6-80f1-4b7e-92b7-784e411b832c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
